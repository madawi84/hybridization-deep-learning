# hybridization-deep-learning
A study on deep learning compiler optimization using hybridization in TensorFlow with a benchmarked implementation.

  # âš¡ Acceleration by Hybridization in Deep Learning

This project demonstrates how hybridization â€” the process of converting imperative code to symbolic computation â€” accelerates the execution of neural networks in TensorFlow. The approach is benchmarked using `eager` vs `graph` mode with `tf.function`.

## ğŸ“˜ Concept

Hybridization combines:
- **Imperative programming** (flexibility)  
- **Symbolic graph execution** (optimization & speed)

By converting Python-based models to TensorFlow graphs using `tf.function`, execution becomes faster and memory-efficient.

## ğŸ§  Key Takeaways

- Implemented a benchmark comparing Eager Mode and Graph Mode
- Used a custom benchmarking class to time 5000 iterations
- Demonstrated significant acceleration using `tf.function`

## ğŸ› ï¸ Tools Used

- Python
- TensorFlow
- Keras
- Colab

## ğŸ§ª Benchmark Results (Example Output)

| Mode        | Time (seconds) |
|-------------|----------------|
| Eager Mode  | 2.37           |
| Graph Mode  | 1.02           |

> *Graph mode execution significantly outperforms eager mode in this setup.*

## ğŸ“‚ Project Structure

hybridization-deep-learning/  
â”‚  
â”œâ”€â”€ hybridization_benchmark.ipynb  
â”œâ”€â”€ README.md  
â””â”€â”€ requirements.txt  

  
## ğŸ“„ Based on

- [Dive into Deep Learning â€“ Hybridization](https://d2l.ai/chapter_computational-performance/hybridize.html#symbolic-programming)
- [TensorFlow tf.function Docs](https://www.tensorflow.org/api_docs/python/tf/function)

## ğŸ‘©â€ğŸ’» Author

Madawi Alsoyohi  
[LinkedIn](https://www.linkedin.com/in/madawi-alsoyohi-7134951a6) | [GitHub](https://github.com/madawi84)
